{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59031e46",
   "metadata": {},
   "source": [
    "Script to train a BG for capped proline - see alanine dipeptide script for more details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "408f8404",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = \"cuda:3\" if torch.cuda.is_available() else \"cpu\"\n",
    "dtype = torch.float32\n",
    "\n",
    "# a context tensor to send data to the right device and dtype via '.to(ctx)'\n",
    "ctx = torch.zeros([], device=device, dtype=dtype)\n",
    "\n",
    "# a brief check if this module is the main executable (or imported)\n",
    "main = (__name__ == \"__main__\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3980b903",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import mdtraj\n",
    "rough_dataset_trans = mdtraj.load('Trajectories/trans_pro_300K_noconstr_long.dcd', top='trans_pro_reindexed.pdb')\n",
    "rough_dataset_cis = mdtraj.load('Trajectories/cis_pro_300K_noconstr_long.dcd',top='cis_pro.pdb')\n",
    "\n",
    "rough_dataset = rough_dataset_trans.join(rough_dataset_cis)\n",
    "\n",
    "dataset = rough_dataset.superpose(rough_dataset_trans[0])\n",
    "\n",
    "#only relevant to retain parameters from MD\n",
    "md_fname = 'trans_pro_300K_long'\n",
    "fname = \"test\"\n",
    "\n",
    "coordinates = dataset.xyz\n",
    "n_atoms = 26\n",
    "\n",
    "#defining parameters\n",
    "from simtk import openmm, unit\n",
    "import pickle\n",
    "try:\n",
    "    pickleFile = open(f'parameters/parameters{md_fname}.pkl','rb')\n",
    "    parametersdict = pickle.load(pickleFile)\n",
    "    temperature = parametersdict['Temperature']\n",
    "    collision_rate = parametersdict['Collision rate']\n",
    "    timestep = parametersdict['Timestep']\n",
    "    reportInterval = parametersdict['Report Interval']\n",
    "except:\n",
    "    print('no parameters found')\n",
    "    temperature = 300.0 * unit.kelvin\n",
    "    collision_rate = 1.0 / unit.picosecond\n",
    "    timestep = 2.0 * unit.femtosecond"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fe01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining iterations\n",
    "n_iter_NLL = 20000\n",
    "n_iter_mixed = 20000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ad9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "rigid_block = np.array([9,7,6])\n",
    "z_matrix = np.array([\n",
    "    [0,4,6,7],\n",
    "    [1,0,4,6],\n",
    "    [2,0,4,6],\n",
    "    [3,0,4,6],\n",
    "    [4,6,7,9],\n",
    "    [5,4,6,7],\n",
    "    [8,7,6,4],\n",
    "    [10,9,7,6],\n",
    "    [11,7,9,24],\n",
    "    [12,11,7,6],\n",
    "    [13,11,7,6],\n",
    "    [14,17,6,4],\n",
    "    [15,14,17,6],\n",
    "    [16,14,17,6],\n",
    "    [17,6,7,9],\n",
    "    [18,17,6,7],\n",
    "    [19,17,6,7],\n",
    "    [20,24,9,7],\n",
    "    [21,20,24,9],\n",
    "    [22,20,24,9],\n",
    "    [23,20,24,9],\n",
    "    [24,9,7,6],\n",
    "    [25,24,9,7]\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa890e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dimensions(dataset):\n",
    "        return np.prod(dataset.xyz[0].shape)\n",
    "dim = dimensions(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ed31d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#setting up openmm bridge for energy evaluation\n",
    "from simtk import openmm\n",
    "with open('cis_pro_xmlsystem.txt') as f:\n",
    "    xml = f.read()\n",
    "system = openmm.XmlSerializer.deserialize(xml)\n",
    "\n",
    "from bgflow.distribution.energy.openmm import OpenMMBridge, OpenMMEnergy\n",
    "from openmmtools import integrators\n",
    "\n",
    "integrator = integrators.LangevinIntegrator(temperature=temperature,collision_rate=collision_rate,timestep=timestep)\n",
    "\n",
    "energy_bridge = OpenMMBridge(system, integrator, n_workers=1)\n",
    "target_energy = OpenMMEnergy(int(dim), energy_bridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ac4abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mdtraj as md "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af9b16e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_phi_psi(trajectory):\n",
    "    phi_atoms = [4, 6, 7,9]\n",
    "    phi = md.compute_dihedrals(trajectory, indices=[phi_atoms])[:, 0]\n",
    "    psi_atoms = [6, 7, 9, 24]\n",
    "    psi = md.compute_dihedrals(trajectory, indices=[psi_atoms])[:, 0]\n",
    "    return phi, psi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d5977b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import mdtraj as md \n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "\n",
    "def plot_phi_psi(ax, trajectory):\n",
    "    #plots the Ramachandran plot\n",
    "    if not isinstance(trajectory, md.Trajectory):\n",
    "        trajectory = md.Trajectory(\n",
    "            xyz=trajectory.cpu().detach().numpy().reshape(-1, n_atoms, 3), \n",
    "            topology=md.load('trans_pro_reindexed.pdb').topology\n",
    "        )\n",
    "    phi, psi = compute_phi_psi(trajectory)\n",
    "    \n",
    "    ax.hist2d(phi, psi, 50, norm=LogNorm())\n",
    "    ax.set_xlim(-np.pi, np.pi)\n",
    "    ax.set_ylim(-np.pi, np.pi)\n",
    "    ax.set_xlabel(\"$\\phi$\")\n",
    "    _ = ax.set_ylabel(\"$\\psi$\")\n",
    "    \n",
    "    return trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6262364",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_omega(ax, trajectory):\n",
    "    #plots the omega distribution\n",
    "    if not isinstance(trajectory, md.Trajectory):\n",
    "        trajectory = md.Trajectory(\n",
    "            xyz=trajectory.cpu().detach().numpy().reshape(-1, n_atoms, 3), \n",
    "            topology=md.load('cis_pro.pdb').topology\n",
    "        )\n",
    "    \n",
    "    omega_atoms = [0,4,6,7]\n",
    "    omega = md.compute_dihedrals(trajectory, indices=[omega_atoms])[:, 0]\n",
    "    ax.hist(omega, bins=40)\n",
    "    ax.set_xlim(-np.pi, np.pi)\n",
    "    ax.set_xlabel(f\"$\\omega$\")\n",
    "    ax.set_ylabel(f\"Count   [#Samples / {len(omega)}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b356c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1,2,figsize=(6,3))\n",
    "trajectory_md = plot_phi_psi(axes[0], dataset)\n",
    "plot_omega(axes[1],dataset)\n",
    "plt.savefig(f'BG_training_plots/datasetRplots/Rplot{fname}.png',bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbf1d57a",
   "metadata": {},
   "source": [
    "## Split Data and Randomly Permute Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223dd682",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train = len(dataset)//2\n",
    "n_test = len(dataset) - n_train\n",
    "permutation = np.random.permutation(n_train)\n",
    "\n",
    "#different to ala2 - to ensure the cis and trans parts of the dataset are separated\n",
    "all_data = coordinates.reshape(-1, dimensions(dataset))\n",
    "np.random.shuffle(all_data)\n",
    "training_data = torch.tensor(all_data[permutation]).to(ctx)\n",
    "test_data = torch.tensor(all_data[permutation + n_train]).to(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583e162b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bgflow as bg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827928aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# throw away 6 degrees of freedom (rotation and translation)\n",
    "dim_cartesian = len(rigid_block) * 3 - 6\n",
    "dim_bonds = len(z_matrix)\n",
    "dim_angles = dim_bonds\n",
    "dim_torsions = dim_bonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "913771d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "coordinate_transform = bg.MixedCoordinateTransformation(\n",
    "    data=training_data, \n",
    "    z_matrix=z_matrix,\n",
    "    fixed_atoms=rigid_block,\n",
    "    #keepdims=None,\n",
    "    keepdims=dim_cartesian, \n",
    "    normalize_angles=True,\n",
    ").to(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d74bb02",
   "metadata": {},
   "source": [
    "For demonstration, we transform the first 3 samples from the training data set into internal coordinates as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed62be2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bonds, angles, torsions, cartesian, dlogp = coordinate_transform.forward(training_data[:3])\n",
    "bonds.shape, angles.shape, torsions.shape, cartesian.shape, dlogp.shape\n",
    "#print(bonds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1cc5cf4",
   "metadata": {},
   "source": [
    "## Prior Distribution\n",
    "\n",
    "The next step is to define a prior distribution that we can easily sample from. The normalizing flow will be trained to transform such latent samples into molecular coordinates. Here, we just take a normal distribution, which is a rather naive choice for reasons that will be discussed in other notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65e0528a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dim_ics = dim_bonds + dim_angles + dim_torsions + dim_cartesian\n",
    "mean = torch.zeros(dim_ics).to(ctx) \n",
    "# passing the mean explicitly to create samples on the correct device\n",
    "prior = bg.NormalDistribution(dim_ics, mean=mean)\n",
    "print(prior.sample(1).size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fced70d5",
   "metadata": {},
   "source": [
    "## Normalizing Flow\n",
    "\n",
    "Next, we set up the normalizing flow by stacking together different neural networks. For now, we will do this in a rather naive way, not distinguishing between bonds, angles, and torsions. Therefore, we will first define a flow that splits the output from the prior into the different IC terms.\n",
    "\n",
    "### Split Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b02bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "split_into_ics_flow = bg.SplitFlow(dim_bonds, dim_angles, dim_torsions, dim_cartesian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f77249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "ics = split_into_ics_flow(prior.sample(1))\n",
    "coordinate_transform.forward(*ics, inverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdda0c60",
   "metadata": {},
   "source": [
    "### Coupling Layers\n",
    "\n",
    "Next, we will set up so-called RealNVP coupling layers, which split the input into two channels and then learn affine transformations of channel 1 conditioned on channel 2. Here we will do the split naively between the first and second half of the degrees of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9887da8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RealNVP(bg.SequentialFlow):\n",
    "    \n",
    "    def __init__(self, dim, hidden):\n",
    "        self.dim = dim\n",
    "        self.hidden = hidden\n",
    "        super().__init__(self._create_layers())\n",
    "    \n",
    "    def _create_layers(self):\n",
    "        dim_channel1 =  self.dim//2\n",
    "        dim_channel2 = self.dim - dim_channel1\n",
    "        split_into_2 = bg.SplitFlow(dim_channel1, dim_channel2)\n",
    "        \n",
    "        layers = [\n",
    "            # -- split\n",
    "            split_into_2,\n",
    "            # --transform\n",
    "            self._coupling_block(dim_channel1, dim_channel2),\n",
    "            bg.SwapFlow(),\n",
    "            self._coupling_block(dim_channel2, dim_channel1),\n",
    "            # -- merge\n",
    "            bg.InverseFlow(split_into_2)\n",
    "        ]\n",
    "        return layers\n",
    "        \n",
    "    def _dense_net(self, dim1, dim2):\n",
    "        return bg.DenseNet(\n",
    "            [dim1, *self.hidden, dim2],\n",
    "            activation=torch.nn.ReLU()\n",
    "        )\n",
    "    \n",
    "    def _coupling_block(self, dim1, dim2):\n",
    "        return bg.CouplingFlow(bg.AffineTransformer(\n",
    "            shift_transformation=self._dense_net(dim1, dim2),\n",
    "            scale_transformation=self._dense_net(dim1, dim2)\n",
    "        ))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65cbae6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "RealNVP(dim_ics, hidden=[128]).to(ctx).forward(prior.sample(3))[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c4395e",
   "metadata": {},
   "source": [
    "### Boltzmann Generator\n",
    "\n",
    "Finally, we define the Boltzmann generator.\n",
    "It will sample molecular conformations by \n",
    "\n",
    "1. sampling in latent space from the normal prior distribution,\n",
    "2. transforming the samples into a more complication distribution through a number of RealNVP blocks (the parameters of these blocks will be subject to optimization),\n",
    "3. splitting the output of the network into blocks that define the internal coordinates, and\n",
    "4. transforming the internal coordinates into Cartesian coordinates through the inverse IC transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0fb373",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_realnvp_blocks = 5\n",
    "layers = []\n",
    "\n",
    "for i in range(n_realnvp_blocks):\n",
    "    layers.append(RealNVP(dim_ics, hidden=[128, 128, 128]))\n",
    "layers.append(split_into_ics_flow)\n",
    "layers.append(bg.InverseFlow(coordinate_transform))\n",
    "\n",
    "flow = bg.SequentialFlow(layers).to(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430095ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "#print(flow.forward(prior.sample(6)))\n",
    "\n",
    "flow.forward(training_data[:3], inverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f646e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print number of trainable parameters\n",
    "\"#Parameters:\", np.sum([np.prod(p.size()) for p in flow.parameters()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3741b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = bg.BoltzmannGenerator(\n",
    "    flow=flow,\n",
    "    prior=prior,\n",
    "    target=target_energy\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3cb2f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_energies(ax, samples, target_energy, test_data):\n",
    "    sample_energies = target_energy.energy(samples).cpu().detach().numpy()\n",
    "    md_energies = target_energy.energy(test_data[:len(samples)]).cpu().detach().numpy()\n",
    "    cut = max(np.percentile(sample_energies, 80), 20)\n",
    "    \n",
    "    ax.set_xlabel(\"Energy   [$k_B T$]\")\n",
    "    # y-axis on the right\n",
    "    ax2 = plt.twinx(ax)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "    ax2.hist(sample_energies, range=(-50, cut), bins=40, density=False, label=\"BG\")\n",
    "    ax2.hist(md_energies, range=(-50, cut), bins=40, density=False, label=\"MD\")\n",
    "    ax2.set_ylabel(f\"Count   [#Samples / {len(samples)}]\")\n",
    "    ax2.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce8458c",
   "metadata": {},
   "source": [
    "## Train\n",
    "\n",
    "Boltzmann generators can be trained in two ways:\n",
    "1. by matching the density of samples from the training data via the negative log likelihood (NLL), and\n",
    "2. by matching the target density via the backward Kullback-Leibler loss (KLL).\n",
    "\n",
    "NLL-based training is faster, as it does not require the computation of molecular target energies. Therefore, we will first train the generator solely by density estimation.\n",
    "\n",
    "### NLL Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50db04f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nll_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-3)\n",
    "nll_trainer = bg.KLTrainer(\n",
    "    generator, \n",
    "    optim=nll_optimizer,\n",
    "    train_energy=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897fe02b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "nll_trainer.train(\n",
    "    n_iter=n_iter_NLL, \n",
    "    data=training_data,\n",
    "    batchsize=128,\n",
    "    n_print=1000, \n",
    "    w_energy=0.0\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7122439",
   "metadata": {},
   "source": [
    "To see what the generator has learned so far, let's first create a bunch of samples and compare their backbone angles with the molecular dynamics data. Let's also plot their energies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc600af",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "samples = generator.sample(n_samples)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,3))\n",
    "fig.tight_layout()\n",
    "\n",
    "plot_phi_psi(axes[0], samples)\n",
    "plot_omega(axes[1],samples)\n",
    "plot_energies(axes[2], samples, target_energy, test_data)\n",
    "plt.savefig(f\"BG_training_plots/{fname}NLL_learnt.png\")\n",
    "\n",
    "del samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef271578",
   "metadata": {},
   "source": [
    "### Mixed Training\n",
    "\n",
    "The next step is \"mixed\" training with a combination of NLL and KLL. To retain some of the progress made in the NLL phase, we decrease the learning rate and increase the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eef8c0da",
   "metadata": {},
   "outputs": [],
   "source": [
    "mixed_optimizer = torch.optim.Adam(generator.parameters(), lr=1e-4)\n",
    "mixed_trainer = bg.KLTrainer(\n",
    "    generator, \n",
    "    optim=mixed_optimizer,\n",
    "    train_energy=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be7ae97",
   "metadata": {},
   "source": [
    "Mixed training will be considerably slower. \n",
    "To speed it up, you can change the settings for the OpenMM energy when creating the energy model. For example, consider not passing `n_workers=1`.\n",
    "\n",
    "To avoid large potential energy gradients from singularities, the components of the KL gradient are constrained to (-100, 100). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc524fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "mixed_trainer.train(\n",
    "    n_iter=n_iter_mixed, \n",
    "    data=training_data,\n",
    "    batchsize=1000,\n",
    "    n_print=100, \n",
    "    w_energy=0.1,\n",
    "    w_likelihood=0.9,\n",
    "    clip_forces=20.0\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ac2152",
   "metadata": {},
   "source": [
    "Plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e926d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(flow.state_dict(),f'model{fname}.pt')\n",
    "#torch.save(generator.state_dict(keep_vars=True),f'model{fname}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d319fd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 10000\n",
    "samples = generator.sample(n_samples)\n",
    "#print(samples)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12,4))\n",
    "fig.tight_layout()\n",
    "\n",
    "samplestrajectory = plot_phi_psi(axes[0], samples)\n",
    "plot_omega(axes[1],samples)\n",
    "plot_energies(axes[2], samples, target_energy, test_data)\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"BG_training_plots/{fname}KLL_learnt.png\", bbox_inches = 'tight')\n",
    "\n",
    "\n",
    "samplestrajectory.save(f\"Trajectories/{fname}_samplestraj.dcd\")\n",
    "\n",
    "#del samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "557097de",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "This tutorial has introduced the most basic concepts and implementations underlying Boltzmann generators and `bgflow`. That said, the trained networks did not do a particularly good job in reproducing the molecular Boltzmann distribution. Specifically, they only modeled the major modes of the $\\phi$ angle and still produced many samples with unreasonably large energies. Let's look at a few shortcomings of the present architecture:\n",
    "\n",
    "### 1) Unconstrained Internal Coordinates\n",
    "Bonds, angles, and torsions must not take arbitrary values in principle. Bond lengths need to be positive, angles live in $[0,\\pi],$ and torsions are periodic in $[-\\pi, \\pi].$ Neither those bounds nor the periodicity of torsions distributions have been taken into account by the present Boltzmann generator. The layers of the normalizing flow should be build in a way that preserves these constraints on the ICs.\n",
    "\n",
    "### 2)  Arbitrary Coupling\n",
    "The input for the coupling layers was split into two channels rather arbitrarily (first vs. second half). A partial remedy is to define the conditioning in a physically informed manner. Another solution is to augment the base space by momenta, which can be done with augmented normalizing flows (see for instance the notebook on temperature-steering flows).\n",
    "\n",
    "### 3) RealNVP Layers\n",
    "Affine coupling layers are well-known to perform poorly in separating modes. This explains that the metastable region around $\\phi \\approx \\pi/2$ was not captured by the generator. Other architectures such as augmented flows or neural spline flows do a better job for complicated, multimodal distributions.\n",
    "\n",
    "### 4) Training\n",
    "The generators were only trained for relatively few iterations and performance may improve with longer training and better choices of the learning rate and hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9be966",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7c9d31eb73c8f3e112db66097c42d16831eaf5100aebfeaf3802cb7e3312826a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('bgflow_env': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
